{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torchinfo\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformerAEclass import *\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path):\n",
    "    \"\"\"Loads a dataset from a CSV file.\"\"\"\n",
    "    return pd.read_csv(dataset_path)\n",
    "\n",
    "def filter_dataset_by_cases_and_channels(dataset, cases, channels):\n",
    "    \"\"\"Filters a dataset to keep only the rows that correspond to the specified cases and channels.\"\"\"\n",
    "    selected_rows = pd.DataFrame()\n",
    "    for case_number in cases:\n",
    "        rows_for_case = dataset[dataset['Case'] == f'case{case_number}']\n",
    "        selected_rows = pd.concat([selected_rows, rows_for_case])\n",
    "    selected_rows = selected_rows[selected_rows['Channel'].isin(channels)]\n",
    "    return selected_rows\n",
    "\n",
    "def split_dataset_for_train_val_test(data_pd):\n",
    "    \"\"\"Splits a dataset into three parts: train, validation, and test.\"\"\"\n",
    "    normal_data = data_pd[data_pd['norm/ab'] == 'normal']\n",
    "    abnormal_data = data_pd[data_pd['norm/ab'] == 'abnormal']\n",
    "    \n",
    "    # We only need normal data for training, but validation and test need both normal and abnormal data.\n",
    "    train_data, intermediate_data = train_test_split(normal_data, test_size=0.2, shuffle=True)\n",
    "    validation_data, test_data = train_test_split(pd.concat([abnormal_data, intermediate_data]), test_size=0.8, shuffle=True)\n",
    "\n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "def train_val_test(dataset_path=None, cases=[], channels=[]):\n",
    "    \"\"\"Loads a dataset, filters it, and splits it for training, validation, and test.\"\"\"\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    filtered_dataset = filter_dataset_by_cases_and_channels(dataset, cases, channels)\n",
    "    train_data, validation_data, test_data = split_dataset_for_train_val_test(filtered_dataset)\n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "\n",
    "datapath = r'C:\\Users\\brech\\THESIS_local\\ToyADMOS\\ToycarCSV.csv'\n",
    "cases = [1]\n",
    "channels = ['ch1']\n",
    "train_data, validation_data, test_data = train_val_test(dataset_path=datapath, cases=cases, channels=channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (1080, 7) \n",
      " validation shape:  (106, 7) \n",
      " test_dataset:  (428, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"train shape: \", train_data.shape, \"\\n\", \"validation shape: \", validation_data.shape, \"\\n\", \"test_dataset: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions \n",
    "def find_path_to_wav(full_sample_name):\n",
    "    for root, dirs, files in os.walk(os.path.dirname(datapath)):\n",
    "        for name in files:\n",
    "            if name == full_sample_name:\n",
    "                path_to_wavFile = os.path.abspath(os.path.join(root, name))\n",
    "                return path_to_wavFile\n",
    "\n",
    "\n",
    "def get_sample_waveform_normalised(full_sample_name, start = 0, stop = 11):\n",
    "    #returns waveform values, cut to seconds going from start to stop\n",
    "    sample_path = find_path_to_wav(full_sample_name)\n",
    "    waveform, sample_rate = librosa.load(sample_path, sr= None)\n",
    "    waveform = waveform[int(start*sample_rate): int(stop*sample_rate)]\n",
    "        \n",
    "    return librosa.util.normalize(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wav = train_data[\"Full Sample Name\"].values\n",
    "X_test_wav = test_data[\"Full Sample Name\"].values\n",
    "X_valid_wav = validation_data[\"Full Sample Name\"].values\n",
    "\n",
    "batch_train = np.array([get_sample_waveform_normalised(elem,4,4.5) for elem in X_train_wav]) \n",
    "batch_test = np.array([get_sample_waveform_normalised(elem,4,4.5) for elem in X_test_wav])\n",
    "batch_val = np.array([get_sample_waveform_normalised(elem,4,4.5) for elem in X_valid_wav])\n",
    "\n",
    "batch_train_reshaped =  np.reshape(batch_train,(len(batch_train),1,8000,1))\n",
    "batch_test_reshaped =  np.reshape(batch_test,(len(batch_test),1,8000,1))\n",
    "batch_val_reshaped =  np.reshape(batch_val,(len(batch_val),1,8000,1))\n",
    "\n",
    "\n",
    "X_train = DataLoader(batch_train_reshaped, batch_size=32, shuffle=False)  # comes from 64\n",
    "X_test = DataLoader(batch_test_reshaped, batch_size=32, shuffle=False)\n",
    "X_val = DataLoader(batch_val_reshaped, batch_size=32, shuffle=False)\n",
    "\n",
    "Y_train = train_data[\"norm/ab\"]\n",
    "Y_train = np.array([1 if i == \"normal\" else -1 for i in Y_train]).reshape(-1, 1)\n",
    "\n",
    "Y_val = validation_data[\"norm/ab\"]\n",
    "Y_val = np.array([1 if i == \"normal\" else -1 for i in Y_val]).reshape(-1, 1)\n",
    "\n",
    "Y_test = test_data[\"norm/ab\"]\n",
    "Y_test = np.array([1 if i == \"normal\" else -1 for i in Y_test]).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beforesqueeze torch.Size([1, 512, 250, 1])\n",
      "aftersqueeze torch.Size([1, 512, 250])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Transformer                              [1, 1, 8000, 1]           --\n",
       "├─AudioEmbedding: 1-1                    [1, 512, 250]             --\n",
       "│    └─Sequential: 2-1                   [1, 32, 4000, 1]          --\n",
       "│    │    └─Conv2d: 3-1                  [1, 32, 8000, 1]          128\n",
       "│    │    └─Tanh: 3-2                    [1, 32, 8000, 1]          --\n",
       "│    │    └─MaxPool2d: 3-3               [1, 32, 4000, 1]          --\n",
       "│    └─Sequential: 2-2                   [1, 64, 2000, 1]          --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 4000, 1]          6,208\n",
       "│    │    └─Tanh: 3-5                    [1, 64, 4000, 1]          --\n",
       "│    │    └─MaxPool2d: 3-6               [1, 64, 2000, 1]          --\n",
       "│    └─Sequential: 2-3                   [1, 128, 1000, 1]         --\n",
       "│    │    └─Conv2d: 3-7                  [1, 128, 2000, 1]         24,704\n",
       "│    │    └─Tanh: 3-8                    [1, 128, 2000, 1]         --\n",
       "│    │    └─MaxPool2d: 3-9               [1, 128, 1000, 1]         --\n",
       "│    └─Sequential: 2-4                   [1, 256, 500, 1]          --\n",
       "│    │    └─Conv2d: 3-10                 [1, 256, 1000, 1]         98,560\n",
       "│    │    └─Tanh: 3-11                   [1, 256, 1000, 1]         --\n",
       "│    │    └─MaxPool2d: 3-12              [1, 256, 500, 1]          --\n",
       "│    └─Sequential: 2-5                   [1, 512, 250, 1]          --\n",
       "│    │    └─Conv2d: 3-13                 [1, 512, 500, 1]          393,728\n",
       "│    │    └─Tanh: 3-14                   [1, 512, 500, 1]          --\n",
       "│    │    └─MaxPool2d: 3-15              [1, 512, 250, 1]          --\n",
       "│    └─Dropout: 2-6                      [1, 512, 250, 1]          --\n",
       "├─Sequential: 1-2                        [1, 512, 250]             --\n",
       "│    └─TransformerBlock: 2-7             [1, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-16          [1, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-17              [1, 512, 250]             500\n",
       "│    │    └─Sequential: 3-18             [1, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-19              [1, 512, 250]             500\n",
       "│    └─TransformerBlock: 2-8             [1, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-20          [1, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-21              [1, 512, 250]             500\n",
       "│    │    └─Sequential: 3-22             [1, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-23              [1, 512, 250]             500\n",
       "│    └─TransformerBlock: 2-9             [1, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-24          [1, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-25              [1, 512, 250]             500\n",
       "│    │    └─Sequential: 3-26             [1, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-27              [1, 512, 250]             500\n",
       "│    └─TransformerBlock: 2-10            [1, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-28          [1, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-29              [1, 512, 250]             500\n",
       "│    │    └─Sequential: 3-30             [1, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-31              [1, 512, 250]             500\n",
       "│    └─TransformerBlock: 2-11            [1, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-32          [1, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-33              [1, 512, 250]             500\n",
       "│    │    └─Sequential: 3-34             [1, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-35              [1, 512, 250]             500\n",
       "│    └─TransformerBlock: 2-12            [1, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-36          [1, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-37              [1, 512, 250]             500\n",
       "│    │    └─Sequential: 3-38             [1, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-39              [1, 512, 250]             500\n",
       "├─deconvEmbedding: 1-3                   [1, 1, 8000, 1]           --\n",
       "│    └─Sequential: 2-13                  [1, 256, 500, 1]          --\n",
       "│    │    └─ConvTranspose2d: 3-40        [1, 256, 500, 1]          262,400\n",
       "│    │    └─Tanh: 3-41                   [1, 256, 500, 1]          --\n",
       "│    └─Sequential: 2-14                  [1, 128, 1000, 1]         --\n",
       "│    │    └─ConvTranspose2d: 3-42        [1, 128, 1000, 1]         65,664\n",
       "│    │    └─Tanh: 3-43                   [1, 128, 1000, 1]         --\n",
       "│    └─Sequential: 2-15                  [1, 64, 2000, 1]          --\n",
       "│    │    └─ConvTranspose2d: 3-44        [1, 64, 2000, 1]          16,448\n",
       "│    │    └─Tanh: 3-45                   [1, 64, 2000, 1]          --\n",
       "│    └─Sequential: 2-16                  [1, 32, 4000, 1]          --\n",
       "│    │    └─ConvTranspose2d: 3-46        [1, 32, 4000, 1]          4,128\n",
       "│    │    └─Tanh: 3-47                   [1, 32, 4000, 1]          --\n",
       "│    └─Sequential: 2-17                  [1, 1, 8000, 1]           --\n",
       "│    │    └─ConvTranspose2d: 3-48        [1, 1, 8000, 1]           65\n",
       "│    │    └─Tanh: 3-49                   [1, 1, 8000, 1]           --\n",
       "==========================================================================================\n",
       "Total params: 5,387,033\n",
       "Trainable params: 5,387,033\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 622.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 81.98\n",
       "Params size (MB): 21.55\n",
       "Estimated Total Size (MB): 103.56\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(emb_dim=250, heads=5, nb_transformer_blocks=6, seq_length=512).to(device=device)\n",
    "torchinfo.summary(model, input_size=(1,1, 8000, 1)) #batch_size, channel, rows,cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = nn.MSELoss()    #?nn.L1Loss() best type of loss for sound?, MSE loss seems to result in lower loss\n",
    "learning_rate = 0.0001  #0.0001 seems best so far\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "losses = []\n",
    "avg_val_losses = []\n",
    "\n",
    "\n",
    "def train(epochs, model, model_loss):\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        \n",
    "        for batch_idx, data in enumerate(X_train):\n",
    "            model.train(True)\n",
    "            # Zero your gradients for every batch!\n",
    "            model.zero_grad()\n",
    "            \n",
    "            #for param in model.parameters(): #instead of model.zero_grad: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#:~:text=implement%20this%20optimization.-,Use%20parameter.grad,-%3D%20None%20instead%20of\n",
    "            #    param.grad = None\n",
    "            \n",
    "            # Make predictions for this batch\n",
    "            data_gpu = data.to(device= device)\n",
    "            outputs = model(data_gpu)\n",
    "    \n",
    "            # Compute the loss and its gradients\n",
    "            loss = model_loss(outputs, data_gpu)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "           \n",
    "            losses.append(loss.item())\n",
    "            #del loss \n",
    "            #del data #free memory\n",
    "            #del outputs\n",
    "            \n",
    "            model.train(False)\n",
    "\n",
    "        \n",
    "        #hier validation data gebruiken, gets run once per epoch\n",
    "        #get average loss value of the validation data\n",
    "        running_val_loss = []\n",
    "        for val_data in X_val:\n",
    "            val_data_gpu = val_data.to(device=device)\n",
    "            val_outputs = model(val_data_gpu)\n",
    "            val_loss = model_loss(val_outputs, val_data_gpu)\n",
    "            \n",
    "            running_val_loss.append(val_loss.item())\n",
    "\n",
    "        avg_val_losses.append(np.average(running_val_loss))\n",
    "\n",
    "train(model=model, epochs=epochs, model_loss=model_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(losses)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(avg_val_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(dataset, scoring_function): \n",
    "    scores_normal = [] #scores of each waveform in the test datase\n",
    "    scores_abnormal = []\n",
    "    \n",
    "    for line_of_data in dataset.iloc():\n",
    "        waveform = np.array(get_sample_waveform_normalised(line_of_data[\"Full Sample Name\"], 4, 4.5))\n",
    "        waveform = np.reshape(waveform,(-1, 1,8000,1))\n",
    "        waveform_gpu = torch.FloatTensor(waveform).to(device=device)\n",
    "\n",
    "        predicted_waveform = model(waveform_gpu)\n",
    "        error = scoring_function(predicted_waveform,waveform_gpu) \n",
    "        \n",
    "        if line_of_data[\"norm/ab\"] == \"normal\":\n",
    "            scores_normal.append(error.detach().cpu().numpy().item()) \n",
    "        \n",
    "        if line_of_data[\"norm/ab\"] == \"abnormal\":\n",
    "            scores_abnormal.append(error.detach().cpu().numpy().item()) \n",
    "   \n",
    "    return scores_normal, scores_abnormal\n",
    "\n",
    "MSE_scores_normal, MSE_scores_abnormal = score(test_data, scoring_function = nn.MSELoss())\n",
    "L1_scores_normal, L1_scores_abnormal = score(test_data, scoring_function = nn.L1Loss())\n",
    "CEL_scores_normal, CEL_scores_abnormal =score(test_data, scoring_function =nn.CrossEntropyLoss()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#RESCALE DATA! met standardscaler\n",
    "L1_scores_normal = np.array(L1_scores_normal).reshape(-1, 1)\n",
    "L1_scores_abnormal = np.array(L1_scores_abnormal).reshape(-1, 1)\n",
    "\n",
    "scaler_normal = StandardScaler() #necessary?\n",
    "scaler_normal.fit_transform(L1_scores_normal)\n",
    "\n",
    "scaler_abnormal = StandardScaler()\n",
    "scaler_abnormal.fit_transform(L1_scores_abnormal)\n",
    "\n",
    "L1_all_scores = np.append(L1_scores_abnormal, L1_scores_normal).reshape(-1, 1) # first abnormal(-1), then normal(1) # test scores\n",
    "L1_all_results = np.ravel(np.concatenate((np.ones_like(L1_scores_abnormal)*(-1), np.ones_like(L1_scores_normal)), axis=0)) #true result\n",
    "\n",
    "# confusion matrix and ROC curve\n",
    "fpr, tpr, _ = roc_curve(L1_all_results,L1_all_scores )  #y_true, y_score\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=3, label=\"ROC curve (area = %0.2f)\" % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=3, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "820d507683270d6b65f843676203e5ed0fadcdee93b1b6d49e394f227c8eaef4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
