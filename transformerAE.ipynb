{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchinfo\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformerAEclass import *\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 250])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Transformer                              [2, 1, 8000, 1]           --\n",
       "├─AudioEmbedding: 1-1                    [2, 512, 250]             --\n",
       "│    └─Sequential: 2-1                   [2, 32, 4000, 1]          --\n",
       "│    │    └─Conv2d: 3-1                  [2, 32, 8000, 1]          128\n",
       "│    │    └─Tanh: 3-2                    [2, 32, 8000, 1]          --\n",
       "│    │    └─MaxPool2d: 3-3               [2, 32, 4000, 1]          --\n",
       "│    └─Sequential: 2-2                   [2, 64, 2000, 1]          --\n",
       "│    │    └─Conv2d: 3-4                  [2, 64, 4000, 1]          6,208\n",
       "│    │    └─Tanh: 3-5                    [2, 64, 4000, 1]          --\n",
       "│    │    └─MaxPool2d: 3-6               [2, 64, 2000, 1]          --\n",
       "│    └─Sequential: 2-3                   [2, 128, 1000, 1]         --\n",
       "│    │    └─Conv2d: 3-7                  [2, 128, 2000, 1]         24,704\n",
       "│    │    └─Tanh: 3-8                    [2, 128, 2000, 1]         --\n",
       "│    │    └─MaxPool2d: 3-9               [2, 128, 1000, 1]         --\n",
       "│    └─Sequential: 2-4                   [2, 256, 500, 1]          --\n",
       "│    │    └─Conv2d: 3-10                 [2, 256, 1000, 1]         98,560\n",
       "│    │    └─Tanh: 3-11                   [2, 256, 1000, 1]         --\n",
       "│    │    └─MaxPool2d: 3-12              [2, 256, 500, 1]          --\n",
       "│    └─Sequential: 2-5                   [2, 512, 250, 1]          --\n",
       "│    │    └─Conv2d: 3-13                 [2, 512, 500, 1]          393,728\n",
       "│    │    └─Tanh: 3-14                   [2, 512, 500, 1]          --\n",
       "│    │    └─MaxPool2d: 3-15              [2, 512, 250, 1]          --\n",
       "│    └─Dropout: 2-6                      [2, 512, 250, 1]          --\n",
       "├─Sequential: 1-2                        [2, 512, 250]             --\n",
       "│    └─TransformerBlock: 2-7             [2, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-16          [2, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-17              [2, 512, 250]             500\n",
       "│    │    └─Sequential: 3-18             [2, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-19              [2, 512, 250]             500\n",
       "│    └─TransformerBlock: 2-8             [2, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-20          [2, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-21              [2, 512, 250]             500\n",
       "│    │    └─Sequential: 3-22             [2, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-23              [2, 512, 250]             500\n",
       "│    └─TransformerBlock: 2-9             [2, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-24          [2, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-25              [2, 512, 250]             500\n",
       "│    │    └─Sequential: 3-26             [2, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-27              [2, 512, 250]             500\n",
       "│    └─TransformerBlock: 2-10            [2, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-28          [2, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-29              [2, 512, 250]             500\n",
       "│    │    └─Sequential: 3-30             [2, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-31              [2, 512, 250]             500\n",
       "│    └─TransformerBlock: 2-11            [2, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-32          [2, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-33              [2, 512, 250]             500\n",
       "│    │    └─Sequential: 3-34             [2, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-35              [2, 512, 250]             500\n",
       "│    └─TransformerBlock: 2-12            [2, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-36          [2, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-37              [2, 512, 250]             500\n",
       "│    │    └─Sequential: 3-38             [2, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-39              [2, 512, 250]             500\n",
       "├─deconvEmbedding: 1-3                   [2, 1, 8000, 1]           --\n",
       "│    └─Sequential: 2-13                  [2, 256, 500, 1]          --\n",
       "│    │    └─ConvTranspose2d: 3-40        [2, 256, 500, 1]          262,400\n",
       "│    │    └─Tanh: 3-41                   [2, 256, 500, 1]          --\n",
       "│    └─Sequential: 2-14                  [2, 128, 1000, 1]         --\n",
       "│    │    └─ConvTranspose2d: 3-42        [2, 128, 1000, 1]         65,664\n",
       "│    │    └─Tanh: 3-43                   [2, 128, 1000, 1]         --\n",
       "│    └─Sequential: 2-15                  [2, 64, 2000, 1]          --\n",
       "│    │    └─ConvTranspose2d: 3-44        [2, 64, 2000, 1]          16,448\n",
       "│    │    └─Tanh: 3-45                   [2, 64, 2000, 1]          --\n",
       "│    └─Sequential: 2-16                  [2, 32, 4000, 1]          --\n",
       "│    │    └─ConvTranspose2d: 3-46        [2, 32, 4000, 1]          4,128\n",
       "│    │    └─Tanh: 3-47                   [2, 32, 4000, 1]          --\n",
       "│    └─Sequential: 2-17                  [2, 1, 8000, 1]           --\n",
       "│    │    └─ConvTranspose2d: 3-48        [2, 1, 8000, 1]           65\n",
       "│    │    └─Tanh: 3-49                   [2, 1, 8000, 1]           --\n",
       "==========================================================================================\n",
       "Total params: 5,387,033\n",
       "Trainable params: 5,387,033\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.24\n",
       "==========================================================================================\n",
       "Input size (MB): 0.06\n",
       "Forward/backward pass size (MB): 163.97\n",
       "Params size (MB): 21.55\n",
       "Estimated Total Size (MB): 185.58\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(emb_dim=250, heads=5, nb_transformer_blocks=6, seq_length=512)\n",
    "torchinfo.summary(model, input_size=(2,1, 8000, 1)) #batch_size, channel, rows,cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path):\n",
    "    \"\"\"Loads a dataset from a CSV file.\"\"\"\n",
    "    return pd.read_csv(dataset_path)\n",
    "\n",
    "def filter_dataset_by_cases_and_channels(dataset, cases, channels):\n",
    "    \"\"\"Filters a dataset to keep only the rows that correspond to the specified cases and channels.\"\"\"\n",
    "    selected_rows = pd.DataFrame()\n",
    "    for case_number in cases:\n",
    "        rows_for_case = dataset[dataset['Case'] == f'case{case_number}']\n",
    "        selected_rows = pd.concat([selected_rows, rows_for_case])\n",
    "    selected_rows = selected_rows[selected_rows['Channel'].isin(channels)]\n",
    "    return selected_rows\n",
    "\n",
    "def split_dataset_for_train_val_test(data_pd):\n",
    "    \"\"\"Splits a dataset into three parts: train, validation, and test.\"\"\"\n",
    "    normal_data = data_pd[data_pd['norm/ab'] == 'normal']\n",
    "    abnormal_data = data_pd[data_pd['norm/ab'] == 'abnormal']\n",
    "    \n",
    "    # We only need normal data for training, but validation and test need both normal and abnormal data.\n",
    "    train_data, intermediate_data = train_test_split(normal_data, test_size=0.2, shuffle=True)\n",
    "    validation_data, test_data = train_test_split(pd.concat([abnormal_data, intermediate_data]), test_size=0.8, shuffle=True)\n",
    "\n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "def train_val_test(dataset_path=None, cases=[], channels=[]):\n",
    "    \"\"\"Loads a dataset, filters it, and splits it for training, validation, and test.\"\"\"\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    filtered_dataset = filter_dataset_by_cases_and_channels(dataset, cases, channels)\n",
    "    train_data, validation_data, test_data = split_dataset_for_train_val_test(filtered_dataset)\n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "\n",
    "datapath = r'C:\\Users\\brech\\THESIS_local\\ToyADMOS\\ToycarCSV.csv'\n",
    "cases = [1]\n",
    "channels = ['ch1']\n",
    "train_data, validation_data, test_data = train_val_test(dataset_path=datapath, cases=cases, channels=channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (1080, 7) \n",
      " validation shape:  (106, 7) \n",
      " test_dataset:  (428, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"train shape: \", train_data.shape, \"\\n\", \"validation shape: \", validation_data.shape, \"\\n\", \"test_dataset: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions \n",
    "def find_path_to_wav(full_sample_name):\n",
    "    for root, dirs, files in os.walk(os.path.dirname(datapath)):\n",
    "        for name in files:\n",
    "            if name == full_sample_name:\n",
    "                path_to_wavFile = os.path.abspath(os.path.join(root, name))\n",
    "                return path_to_wavFile\n",
    "\n",
    "\n",
    "def get_sample_waveform_normalised(full_sample_name, start = 0, stop = 11):\n",
    "    #returns waveform values, cut to seconds going from start to stop\n",
    "    sample_path = find_path_to_wav(full_sample_name)\n",
    "    waveform, sample_rate = librosa.load(sample_path, sr= None)\n",
    "    waveform = waveform[int(start*sample_rate): int(stop*sample_rate)]\n",
    "        \n",
    "    return librosa.util.normalize(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wav = train_data[\"Full Sample Name\"].values\n",
    "X_test_wav = test_data[\"Full Sample Name\"].values\n",
    "X_valid_wav = validation_data[\"Full Sample Name\"].values\n",
    "\n",
    "batch_train = np.array([get_sample_waveform_normalised(elem,4,4.5) for elem in X_train_wav]) \n",
    "batch_test = np.array([get_sample_waveform_normalised(elem,4,4.5) for elem in X_test_wav])\n",
    "batch_val = np.array([get_sample_waveform_normalised(elem,4,4.5) for elem in X_valid_wav])\n",
    "\n",
    "X_train = DataLoader(batch_train, batch_size=64, shuffle=False)\n",
    "X_test = DataLoader(batch_test, batch_size=64, shuffle=False)\n",
    "X_val = DataLoader(batch_val, batch_size=64, shuffle=False)\n",
    "\n",
    "Y_train = train_data[\"norm/ab\"]\n",
    "Y_train = np.array([1 if i == \"normal\" else -1 for i in Y_train]).reshape(-1, 1)\n",
    "\n",
    "Y_val = validation_data[\"norm/ab\"]\n",
    "Y_val = np.array([1 if i == \"normal\" else -1 for i in Y_val]).reshape(-1, 1)\n",
    "\n",
    "Y_test = test_data[\"norm/ab\"]\n",
    "Y_test = np.array([1 if i == \"normal\" else -1 for i in Y_test]).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer().to(device=device)\n",
    "#torchinfo.summary(model) #batch_size, channel, rows,cols\n",
    "\n",
    "\n",
    "model_loss = nn.BCEWithLogitsLoss()    #?nn.L1Loss() best type of loss for sound?, MSE loss seems to result in lower loss\n",
    "learning_rate = 0.0001  #0.0001 seems best so far\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "de embedding moet een vector zijn, maar de waveform kan eigenlijk op zich bekeken worden als een vector van [8000,1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
