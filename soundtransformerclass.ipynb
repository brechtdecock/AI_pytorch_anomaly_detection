{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchinfo\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINING THE TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size=(3,1), stride=1, padding=\"same\"),\n",
    "                                    nn.Tanh(), #default neg slope of 0.01\n",
    "                                    nn.MaxPool2d(kernel_size=(2,1), stride=2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(in_channels = 32,out_channels = 64, kernel_size=(3,1), stride=1, padding=\"same\"),\n",
    "                                   nn.Tanh(),\n",
    "                                   nn.MaxPool2d(kernel_size=(2,1), stride=2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(in_channels = 64,out_channels = 128, kernel_size=(3,1), stride=1, padding=\"same\"),\n",
    "                                   nn.Tanh(),\n",
    "                                   nn.MaxPool2d(kernel_size=(2,1), stride=2))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(in_channels = 128,out_channels = 256, kernel_size=(3,1), stride=1, padding=\"same\"),\n",
    "                                   nn.Tanh(),\n",
    "                                   nn.MaxPool2d(kernel_size=(2,1), stride=2))\n",
    "        self.conv5 = nn.Sequential(nn.Conv2d(in_channels = 256,out_channels = 512, kernel_size=(3,1), stride=1, padding=\"same\"),\n",
    "                                   nn.Tanh(),\n",
    "                                   nn.MaxPool2d(kernel_size=(2,1), stride=2))\n",
    "        #    shape   =  (?, 256, 500, 1)\n",
    "        #    Conv      ->(?, 512, 500, 1)\n",
    "        #    Pool      ->(?, 512, 250, 1)-->squeeze en dan transpose for 250x512\n",
    "        \n",
    "        self.drop  = nn.Dropout(0.1)\n",
    "          \n",
    "    def getPositionEncoding(self,rows, cols,n=10000):\n",
    "        P = torch.zeros((rows, cols))\n",
    "        for k in range(rows):\n",
    "            for i in torch.arange(int(cols/2)):\n",
    "                denominator = torch.pow(n, 2*i/cols)\n",
    "                P[k, 2*i] = torch.sin(k/denominator)\n",
    "                P[k, 2*i+1] = torch.cos(k/denominator)\n",
    "        return P\n",
    "    \n",
    "    \n",
    "        \n",
    "    def forward(self, data):\n",
    "       \n",
    "        out_32_enc = self.conv1(data)\n",
    "        out_64_enc = self.conv2(out_32_enc)\n",
    "        out_128_enc = self.conv3(out_64_enc)\n",
    "        out_256_enc = self.conv4(out_128_enc)\n",
    "        out_512_enc = self.conv5(out_256_enc)\n",
    "      \n",
    "        out_drop = self.drop(out_512_enc)\n",
    "        \n",
    "        #typically the vector is stored horizontally, while the sequence is stored vertically\n",
    "        conv_feature_embedding = torch.squeeze(out_drop) #so length of 512, each a vector of 250 long \n",
    "        \n",
    "        \n",
    "        position_embedding = self.getPositionEncoding(512,250).to(device=conv_feature_embedding.device)\n",
    "        print(torch.add(conv_feature_embedding,position_embedding).shape )\n",
    "        return torch.add(conv_feature_embedding,position_embedding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deconvEmbedding(nn.Module):\n",
    "        def __init__(self):\n",
    "                super().__init__()\n",
    "        #(?, 512, 250, 1) back all the way upto \n",
    "                self.deconv1 = nn.Sequential(nn.ConvTranspose2d(512, 256, kernel_size = (2,1), stride=2),\n",
    "                                        nn.Tanh()\n",
    "                                        )\n",
    "                self.deconv2 = nn.Sequential(nn.ConvTranspose2d(256, 128, kernel_size = (2,1), stride=2),\n",
    "                                        nn.Tanh()\n",
    "                                        )\n",
    "                self.deconv3 = nn.Sequential(nn.ConvTranspose2d(128, 64, kernel_size = (2,1), stride=2),\n",
    "                                        nn.Tanh()\n",
    "                                        )\n",
    "                self.deconv4 = nn.Sequential(nn.ConvTranspose2d(64, 32, kernel_size = (2,1), stride=2),\n",
    "                                        nn.Tanh()\n",
    "                                        )\n",
    "                self.deconv5 = nn.Sequential(nn.ConvTranspose2d(32, 1, kernel_size = (2,1), stride=2),\n",
    "                                        nn.Tanh()\n",
    "                                        )\n",
    "                \n",
    "        def forward(self, x):\n",
    "                out_256_dec = self.deconv1(x)\n",
    "                out_128_dec = self.deconv2(out_256_dec)\n",
    "                out_64_dec = self.deconv3(out_128_dec)\n",
    "                out_32_dec = self.deconv4(out_64_dec)\n",
    "                out_1_dec = self.deconv5(out_32_dec)\n",
    "                return out_1_dec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input is now a 512x250 matrix, with the embedding vectors stored horizontally. Meaning emb_dim == 250 and seq_len = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, heads=5, mask=False): \n",
    "        super().__init__()\n",
    "        \n",
    "        assert emb_dim % heads == 0\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.heads = heads\n",
    "        \n",
    "        # These compute the queries, keys and values for ALL heads\n",
    "        self.to_keys    = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.to_queries = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.to_values  = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "\n",
    "        # This will be applied after the multi-head self-attention operation.\n",
    "        self.unifyheads = nn.Linear(emb_dim, emb_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        batch, seq_len, emb_dim = x.size()\n",
    "        heads = self.heads #double code?\n",
    "\n",
    "        queries = self.to_queries(x)\n",
    "        keys    = self.to_keys(x)   \n",
    "        values  = self.to_values(x)\n",
    "        \n",
    "            \n",
    "        s = emb_dim // heads  #to divide the query(and key and value) matrix that contains ALL head info to the query of a single head\n",
    "\n",
    "        \"\"\"\"torch.view: returns a new tensor with the same data as the self tensor but of a different shape.\n",
    "        This simply reshapes the tensors to add a dimension that iterations over the heads. \n",
    "        For a single vector in our sequence you can think of it as reshaping a vector of \n",
    "        dimension emb_dim into a matrix of (head x emb_dim//head)\n",
    "        \n",
    "        before: all HEADS info: b x seq_len x emb_dim\n",
    "        after: all HEADS info but iterable per head: b x seq_len x heads x (emb_dim//heads)\n",
    "        \"\"\"\n",
    "        keys    = keys.view(batch, seq_len, heads, s)\n",
    "        queries = queries.view(batch, seq_len, heads, s)\n",
    "        values  = values.view(batch, seq_len, heads, s)\n",
    "        \n",
    "        keys = keys.transpose(1, 2).contiguous().view(batch * heads, seq_len, s)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(batch * heads, seq_len, s)\n",
    "        values = values.transpose(1, 2).contiguous().view(batch * heads, seq_len, s)\n",
    "        \n",
    "            \n",
    "        \"\"\"\n",
    "        Dit volgt gewoon de attention formula: https://storrs.io/attention/#:~:text=the%20attention%20operation%3F-,Attention,-Equation\n",
    "        \"\"\"\n",
    "        # Get dot product of queries and keys, and scale\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2)) #not (0,1) since the zero dimesion are the batches\n",
    "        # -- dot has size (batch*heads, seq_len, seq_len) containing raw weights\n",
    "\n",
    "        # scale the dot product\n",
    "        dot = dot / (emb_dim ** (1/2))\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        if mask is used: create upper triangluar matrix of -inf(this will set softmax to zero at those positions)\n",
    "        \"\"\"\n",
    "        indices = torch.triu_indices(seq_len, seq_len, offset=1)\n",
    "        dot[:, indices[0], indices[1]] = float('-inf')\n",
    "\n",
    "        # normalize \n",
    "        dot = F.softmax(dot, dim=2)\n",
    "        # - dot now contains row-wise normalized weights\n",
    "         \n",
    "        # apply the self attention to the values\n",
    "        out = torch.bmm(dot, values).view(batch, heads, seq_len, s)\n",
    "        \n",
    "        # swap heads, seq_len back, unify heads\n",
    "        out = out.transpose(1, 2).contiguous().view(batch, seq_len, s * heads)  #batch x seq_len x emb_dim\n",
    "    \n",
    "        return self.unifyheads(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = SelfAttention(emb_dim, heads=heads)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        We’ve made the relatively arbitrary choice of making the hidden layer of the feedforward 4 times as big as the input and output. \n",
    "        Smaller values may work as well, and save memory, but it should be bigger than the input/output layers.\n",
    "        \"\"\"\n",
    "        self.ff = nn.Sequential(\n",
    "        nn.Linear(emb_dim, 4 * emb_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4 * emb_dim, emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        attended = self.attention(x)\n",
    "        x = self.norm1(attended + x)\n",
    "\n",
    "        fedforward = self.ff(x)\n",
    "        return self.norm2(fedforward + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, emb_dim, heads, nb_transformer_blocks, seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = AudioEmbedding()\n",
    "\n",
    "\t\t# The sequence of transformer blocks that does all the\n",
    "\t\t# heavy lifting\n",
    "        tblocks = []\n",
    "        for i in range(nb_transformer_blocks):\n",
    "            tblocks.append(TransformerBlock(emb_dim=emb_dim, heads=heads))\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "\n",
    "        self.toAudio = deconvEmbedding()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A (b, t) tensor of integer values representing\n",
    "                  words (in some predetermined vocabulary).\n",
    "        :return: A (b, c) tensor of log-probabilities over the\n",
    "                 classes (where c is the nr. of classes).\n",
    "        \"\"\"\n",
    "        token = self.embedding(x) #token == embedded data, includes vectorization and position data\n",
    "\n",
    "        x = self.tblocks(token)\n",
    "        \n",
    "        # Average-pool over the t dimension and project to class\n",
    "        # probabilities\n",
    "        \n",
    "        x = self.toAudio(x.unsqueeze(-1)) ## add a dimension so [batch, 512, 250] is of the expected size [batch, 512, 250,1] \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 250])\n",
      "transformer block output shape=  torch.Size([2, 512, 250])\n",
      "transformer block output shape=  torch.Size([2, 512, 250])\n",
      "transformer block output shape=  torch.Size([2, 512, 250])\n",
      "transformer block output shape=  torch.Size([2, 512, 250])\n",
      "transformer block output shape=  torch.Size([2, 512, 250])\n",
      "transformer block output shape=  torch.Size([2, 512, 250])\n",
      "shape of output transformer torch.Size([2, 512, 250])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Transformer                              [2, 1, 8000, 1]           --\n",
       "├─AudioEmbedding: 1-1                    [2, 512, 250]             --\n",
       "│    └─Sequential: 2-1                   [2, 32, 4000, 1]          --\n",
       "│    │    └─Conv2d: 3-1                  [2, 32, 8000, 1]          128\n",
       "│    │    └─Tanh: 3-2                    [2, 32, 8000, 1]          --\n",
       "│    │    └─MaxPool2d: 3-3               [2, 32, 4000, 1]          --\n",
       "│    └─Sequential: 2-2                   [2, 64, 2000, 1]          --\n",
       "│    │    └─Conv2d: 3-4                  [2, 64, 4000, 1]          6,208\n",
       "│    │    └─Tanh: 3-5                    [2, 64, 4000, 1]          --\n",
       "│    │    └─MaxPool2d: 3-6               [2, 64, 2000, 1]          --\n",
       "│    └─Sequential: 2-3                   [2, 128, 1000, 1]         --\n",
       "│    │    └─Conv2d: 3-7                  [2, 128, 2000, 1]         24,704\n",
       "│    │    └─Tanh: 3-8                    [2, 128, 2000, 1]         --\n",
       "│    │    └─MaxPool2d: 3-9               [2, 128, 1000, 1]         --\n",
       "│    └─Sequential: 2-4                   [2, 256, 500, 1]          --\n",
       "│    │    └─Conv2d: 3-10                 [2, 256, 1000, 1]         98,560\n",
       "│    │    └─Tanh: 3-11                   [2, 256, 1000, 1]         --\n",
       "│    │    └─MaxPool2d: 3-12              [2, 256, 500, 1]          --\n",
       "│    └─Sequential: 2-5                   [2, 512, 250, 1]          --\n",
       "│    │    └─Conv2d: 3-13                 [2, 512, 500, 1]          393,728\n",
       "│    │    └─Tanh: 3-14                   [2, 512, 500, 1]          --\n",
       "│    │    └─MaxPool2d: 3-15              [2, 512, 250, 1]          --\n",
       "│    └─Dropout: 2-6                      [2, 512, 250, 1]          --\n",
       "├─Sequential: 1-2                        [2, 512, 250]             --\n",
       "│    └─TransformerBlock: 2-7             [2, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-16          [2, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-17              [2, 512, 250]             500\n",
       "│    │    └─Sequential: 3-18             [2, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-19              [2, 512, 250]             500\n",
       "│    │    └─LayerNorm: 3-20              [2, 512, 250]             (recursive)\n",
       "│    └─TransformerBlock: 2-8             [2, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-21          [2, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-22              [2, 512, 250]             500\n",
       "│    │    └─Sequential: 3-23             [2, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-24              [2, 512, 250]             500\n",
       "│    │    └─LayerNorm: 3-25              [2, 512, 250]             (recursive)\n",
       "│    └─TransformerBlock: 2-9             [2, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-26          [2, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-27              [2, 512, 250]             500\n",
       "│    │    └─Sequential: 3-28             [2, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-29              [2, 512, 250]             500\n",
       "│    │    └─LayerNorm: 3-30              [2, 512, 250]             (recursive)\n",
       "│    └─TransformerBlock: 2-10            [2, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-31          [2, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-32              [2, 512, 250]             500\n",
       "│    │    └─Sequential: 3-33             [2, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-34              [2, 512, 250]             500\n",
       "│    │    └─LayerNorm: 3-35              [2, 512, 250]             (recursive)\n",
       "│    └─TransformerBlock: 2-11            [2, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-36          [2, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-37              [2, 512, 250]             500\n",
       "│    │    └─Sequential: 3-38             [2, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-39              [2, 512, 250]             500\n",
       "│    │    └─LayerNorm: 3-40              [2, 512, 250]             (recursive)\n",
       "│    └─TransformerBlock: 2-12            [2, 512, 250]             --\n",
       "│    │    └─SelfAttention: 3-41          [2, 512, 250]             250,250\n",
       "│    │    └─LayerNorm: 3-42              [2, 512, 250]             500\n",
       "│    │    └─Sequential: 3-43             [2, 512, 250]             501,250\n",
       "│    │    └─LayerNorm: 3-44              [2, 512, 250]             500\n",
       "│    │    └─LayerNorm: 3-45              [2, 512, 250]             (recursive)\n",
       "├─deconvEmbedding: 1-3                   [2, 1, 8000, 1]           --\n",
       "│    └─Sequential: 2-13                  [2, 256, 500, 1]          --\n",
       "│    │    └─ConvTranspose2d: 3-46        [2, 256, 500, 1]          262,400\n",
       "│    │    └─Tanh: 3-47                   [2, 256, 500, 1]          --\n",
       "│    └─Sequential: 2-14                  [2, 128, 1000, 1]         --\n",
       "│    │    └─ConvTranspose2d: 3-48        [2, 128, 1000, 1]         65,664\n",
       "│    │    └─Tanh: 3-49                   [2, 128, 1000, 1]         --\n",
       "│    └─Sequential: 2-15                  [2, 64, 2000, 1]          --\n",
       "│    │    └─ConvTranspose2d: 3-50        [2, 64, 2000, 1]          16,448\n",
       "│    │    └─Tanh: 3-51                   [2, 64, 2000, 1]          --\n",
       "│    └─Sequential: 2-16                  [2, 32, 4000, 1]          --\n",
       "│    │    └─ConvTranspose2d: 3-52        [2, 32, 4000, 1]          4,128\n",
       "│    │    └─Tanh: 3-53                   [2, 32, 4000, 1]          --\n",
       "│    └─Sequential: 2-17                  [2, 1, 8000, 1]           --\n",
       "│    │    └─ConvTranspose2d: 3-54        [2, 1, 8000, 1]           65\n",
       "│    │    └─Tanh: 3-55                   [2, 1, 8000, 1]           --\n",
       "==========================================================================================\n",
       "Total params: 5,387,033\n",
       "Trainable params: 5,387,033\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.24\n",
       "==========================================================================================\n",
       "Input size (MB): 0.06\n",
       "Forward/backward pass size (MB): 176.26\n",
       "Params size (MB): 21.55\n",
       "Estimated Total Size (MB): 197.87\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(emb_dim=250, heads=5, nb_transformer_blocks=6, seq_length=512)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
